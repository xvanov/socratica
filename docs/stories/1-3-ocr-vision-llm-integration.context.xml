<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>3</storyId>
    <title>OCR/Vision LLM Integration</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-11-03 16:52</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/1-3-ocr-vision-llm-integration.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>student</asA>
    <iWant>the system to automatically extract text from my uploaded image</iWant>
    <soThat>I don't need to manually transcribe problems from photos or screenshots</soThat>
    <tasks>
      <task id="1" title="Create OCR API route">
        <subtasks>
          <subtask>Create `app/api/ocr/route.ts` file</subtask>
          <subtask>Set up OpenAI client with GPT-4 Turbo Vision</subtask>
          <subtask>Implement image upload handling (receive image from client)</subtask>
          <subtask>Implement Vision API call with proper prompt for math problem extraction</subtask>
          <subtask>Extract text from Vision API response</subtask>
          <subtask>Return extracted text to client</subtask>
          <subtask>Handle OpenAI API errors gracefully</subtask>
          <subtask>Implement retry logic with exponential backoff (up to 3 attempts)</subtask>
          <subtask>Handle rate limit errors with appropriate messaging</subtask>
          <subtask>Handle network timeout errors with retry option</subtask>
          <subtask>Log errors to console (dev) or Firebase Analytics (prod)</subtask>
        </subtasks>
      </task>
      <task id="2" title="Integrate OCR with ImageUpload component">
        <subtasks>
          <subtask>Update `ImageUpload` component to call OCR API on image selection</subtask>
          <subtask>Send selected image to `/api/ocr` endpoint</subtask>
          <subtask>Display loading indicator during OCR processing</subtask>
          <subtask>Receive extracted text from API response</subtask>
          <subtask>Display extracted text in text input field for review/editing</subtask>
          <subtask>Allow student to edit extracted text before submitting</subtask>
          <subtask>Update component state to manage OCR flow (upload → processing → extracted text)</subtask>
        </subtasks>
      </task>
      <task id="3" title="Implement error handling for OCR">
        <subtasks>
          <subtask>Handle invalid image format errors (image too blurry, no text detected)</subtask>
          <subtask>Show specific error messages: "Unable to read image. Please try a clearer photo or use text input."</subtask>
          <subtask>Display user-friendly error messages for API errors</subtask>
          <subtask>Provide fallback option to switch to text input if OCR fails</subtask>
          <subtask>Show retry button for failed OCR attempts</subtask>
          <subtask>Handle network errors during OCR with retry mechanism</subtask>
        </subtasks>
      </task>
      <task id="4" title="Create OpenAI client utilities">
        <subtasks>
          <subtask>Create `lib/openai/client.ts` file</subtask>
          <subtask>Set up OpenAI client with API key from environment variables</subtask>
          <subtask>Implement Vision API helper function for OCR</subtask>
          <subtask>Create prompt template for math problem extraction</subtask>
          <subtask>Handle API key validation and error handling</subtask>
          <subtask>Export client and helper functions</subtask>
        </subtasks>
      </task>
      <task id="5" title="Configure environment variables">
        <subtasks>
          <subtask>Add `OPENAI_API_KEY` to `.env.local` (server-side only, not NEXT_PUBLIC_)</subtask>
          <subtask>Document environment variable in README or setup docs</subtask>
          <subtask>Verify API key is loaded correctly in API route</subtask>
          <subtask>Ensure API key is not exposed to client-side</subtask>
        </subtasks>
      </task>
      <task id="6" title="Testing and verification">
        <subtasks>
          <subtask>Test OCR API route with valid image (JPG, PNG, WebP)</subtask>
          <subtask>Test extracted text displays in text input field</subtask>
          <subtask>Test loading indicator shows during OCR processing</subtask>
          <subtask>Test extracted text maintains mathematical notation</subtask>
          <subtask>Test student can edit extracted text before submitting</subtask>
          <subtask>Test error handling for invalid images (blurry, no text)</subtask>
          <subtask>Test retry logic for API errors (up to 3 attempts)</subtask>
          <subtask>Test rate limit error handling</subtask>
          <subtask>Test network timeout error handling</subtask>
          <subtask>Test fallback option to switch to text input</subtask>
          <subtask>Verify error messages are user-friendly and actionable</subtask>
          <subtask>Verify errors are logged correctly (dev console or Firebase Analytics)</subtask>
        </subtasks>
      </task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="1">Image is sent to Vision LLM API (OpenAI GPT-4 Turbo Vision) for text extraction</criterion>
    <criterion id="2">Extracted problem text is displayed in text input field for review/editing</criterion>
    <criterion id="3">Loading indicator shows during OCR processing</criterion>
    <criterion id="4">Extracted text maintains mathematical notation where possible</criterion>
    <criterion id="5">Student can edit extracted text before submitting</criterion>
    <criterion id="6">Error Handling: Handles OpenAI API errors gracefully with user-friendly messages, implements retry logic (up to 3 attempts with exponential backoff), handles rate limit errors with appropriate messaging and retry suggestion, handles network timeout errors with retry option, handles invalid image format errors (image too blurry, no text detected), shows specific error messages: "Unable to read image. Please try a clearer photo or use text input.", logs errors to console (dev) or Firebase Analytics (prod) for debugging, provides fallback option to switch to text input if OCR fails</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="docs/epics.md" title="Epic Breakdown">
        <section>Epic 1: Problem Input - Story 1.3</section>
        <snippet>Story 1.3: OCR/Vision LLM Integration defines the OCR functionality for image upload. As a student, I want the system to automatically extract text from my uploaded image, so that I don't need to manually transcribe problems from photos or screenshots. Includes 6 acceptance criteria covering Vision API integration, extracted text display, loading indicator, mathematical notation preservation, text editing, and comprehensive error handling.</snippet>
      </doc>
      <doc path="docs/architecture.md" title="Architecture - Socratica">
        <section>Epic 1: Problem Input</section>
        <snippet>Epic 1 components include TextInput component, ImageUpload component, and ProblemPreview component located in `components/problem-input/` directory. OCR API route is in `app/api/ocr/route.ts`. OpenAI Vision API (GPT-4 Turbo Vision) is used for OCR image processing. Image parsing must extract problem text accurately from screenshots/photos.</snippet>
      </doc>
      <doc path="docs/architecture.md" title="Architecture - Socratica">
        <section>Project Structure</section>
        <snippet>OCR API route in `app/api/ocr/route.ts` (Next.js App Router API route). OpenAI client utilities in `lib/openai/client.ts` (server-side only). ImageUpload component extends to call OCR API after image selection. TextInput component displays extracted text for review/editing.</snippet>
      </doc>
      <doc path="docs/architecture.md" title="Architecture - Socratica">
        <section>Decision Summary</section>
        <snippet>Vision API: OpenAI Vision (same model) - gpt-4-turbo - Epic 1 - Integrated with GPT-4 Turbo, handles OCR. LLM API: OpenAI GPT-4 Turbo - gpt-4-turbo - Epic 2, Epic 3 - Excellent instruction following, built-in Vision API, cost-effective.</snippet>
      </doc>
      <doc path="docs/PRD.md" title="Product Requirements Document">
        <section>Goalpost 1: Problem Input</section>
        <snippet>MVP requirement: Image upload with OCR/Vision LLM parsing. Image parsing must extract problem text accurately from screenshots/photos. This enables students to upload screenshots or photos of math problems without manual typing.</snippet>
      </doc>
      <doc path="docs/stories/1-2-image-upload-interface.md" title="Story 1.2: Image Upload Interface">
        <section>Dev Agent Record - Completion Notes List</section>
        <snippet>Story 1.2 Implementation Complete - 2025-11-03. Created `components/problem-input/ImageUpload.tsx` component with full functionality including file validation, preview display, error handling, and accessibility features. Component includes comprehensive file validation (MIME type, file extension, file size), image corruption detection, image preview using URL.createObjectURL() with proper cleanup, remove/clear functionality, loading state during image processing, comprehensive error handling with user-friendly messages, and retry functionality. Component is integrated into `app/page.tsx` main interface alongside TextInput. ImageUpload component is ready for OCR integration. Use this component as the foundation for OCR integration.</snippet>
      </doc>
      <doc path="docs/stories/1-1-text-input-interface.md" title="Story 1.1: Text Input Interface">
        <section>Dev Agent Record - Completion Notes List</section>
        <snippet>Story 1.1 Implementation Complete - 2025-11-03. Created `components/problem-input/TextInput.tsx` component with full functionality including multi-line support, submit functionality, and accessibility features. Component is integrated into `app/page.tsx` main interface. TextInput component will display extracted text from OCR for review/editing.</snippet>
      </doc>
      <doc path="docs/stories/0-2-firebase-project-setup.md" title="Story 0.2: Firebase Project Setup">
        <section>Dev Agent Record - Completion Notes List</section>
        <snippet>Firebase SDK v12.5.0 installed and configured. Firebase services available at `lib/firebase/` directory. Firebase Storage is available but not required for this story. Images are sent directly to OCR API route (client-side upload). Future stories may use Firebase Storage for image persistence.</snippet>
      </doc>
    </docs>
    <code>
      <file path="socratica/components/problem-input/ImageUpload.tsx" kind="component" symbol="ImageUpload" reason="Image upload component ready for OCR integration. Component includes file selection, validation, preview display, error handling, and accessibility features. Component uses React useState for state management (file state, preview state, error state). For OCR integration, add state for OCR processing (loading, extracted text, OCR errors). Component has onImageSelect callback that can be extended to call OCR API after image selection.">
        <note>ImageUpload component is fully functional and ready for OCR integration. Component includes comprehensive file validation, image preview, error handling, and retry functionality. Use this component as the foundation for OCR integration.</note>
      </file>
      <file path="socratica/components/problem-input/TextInput.tsx" kind="component" symbol="TextInput" reason="Text input component that will display extracted text from OCR for review/editing. Component accepts multi-line text input via textarea element. Includes submit button and Enter key submission. Component has inputValue state that can be set from OCR extracted text. Component allows student to edit extracted text before submitting.">
        <note>TextInput component is fully functional and ready to receive extracted text from OCR. Component has inputValue state that can be updated from OCR response. Component allows editing before submission.</note>
      </file>
      <file path="socratica/app/page.tsx" kind="component" symbol="Home" reason="Main interface page that coordinates ImageUpload and TextInput components. Currently displays both components with divider. Needs to be updated to coordinate OCR flow: ImageUpload calls OCR API → extracted text displayed in TextInput → student can edit and submit. Component has handleImageSelect callback that currently just logs image selection. This callback should be extended to call OCR API and update TextInput with extracted text.">
        <note>Main interface page coordinates ImageUpload and TextInput components. Component has state for selectedImage that can be used for OCR processing. Component needs to be updated to coordinate OCR flow between ImageUpload and TextInput.</note>
      </file>
      <file path="socratica/app" kind="directory" symbol="app/" reason="Next.js App Router directory. Contains page.tsx (main interface). API route `app/api/ocr/route.ts` needs to be created for OCR functionality. API route will receive image from client, call OpenAI Vision API, and return extracted text.">
        <note>API route directory exists but `app/api/ocr/route.ts` needs to be created. API route should use Next.js App Router API route pattern with POST method.</note>
      </file>
      <file path="socratica/lib" kind="directory" symbol="lib/" reason="Utilities and configurations directory. Contains firebase/ directory. OpenAI client utilities directory `lib/openai/` needs to be created. OpenAI client file `lib/openai/client.ts` needs to be created for OpenAI client setup and Vision API helper functions.">
        <note>Utilities directory exists. OpenAI client utilities directory and files need to be created. OpenAI client should be server-side only (not exposed to client).</note>
      </file>
      <file path="socratica/lib/firebase/storage.ts" kind="service" symbol="storage" reason="Firebase Storage instance available for future image uploads. For this story, images are sent directly to OCR API (client-side upload to API route). Firebase Storage may be used in future stories for image persistence. Not required for this story.">
        <note>Firebase Storage is available but not required for this story. Images are sent directly to OCR API route for processing.</note>
      </file>
      <file path="socratica/package.json" kind="manifest" symbol="package.json" reason="Project dependencies. Contains Next.js 16.0.1, React 19.2.0, TypeScript, Tailwind CSS v4, Firebase SDK v12.5.0. OpenAI SDK (openai) needs to be installed. Add to dependencies: `npm install openai` for OpenAI client integration.">
        <note>OpenAI SDK needs to be installed. Run `npm install openai` to add OpenAI client to dependencies.</note>
      </file>
      <file path="socratica/tsconfig.json" kind="config" symbol="tsconfig.json" reason="TypeScript configuration with strict mode enabled and import alias `@/*` configured. Use for OCR API route and OpenAI client type definitions and imports.">
      </file>
    </code>
    <dependencies>
      <ecosystem name="node">
        <package name="next" version="16.0.1" reason="Next.js framework with App Router. OCR API route uses Next.js App Router API routes pattern (`app/api/ocr/route.ts`). Client components may need 'use client' directive.">
        </package>
        <package name="react" version="19.2.0" reason="React library for UI components. Use React hooks (useState) for OCR processing state management (loading, extracted text, errors) in ImageUpload component.">
        </package>
        <package name="react-dom" version="19.2.0" reason="React DOM renderer for client-side rendering.">
        </package>
        <package name="typescript" version="^5" reason="TypeScript language for type safety. Strict mode enabled. Use for OCR API route and OpenAI client type definitions (OCRResponse, OCRError, etc.).">
        </package>
        <package name="tailwindcss" version="^4" reason="Tailwind CSS v4 for utility-first styling. Use Tailwind utility classes following existing patterns from ImageUpload and TextInput components.">
        </package>
        <package name="firebase" version="^12.5.0" reason="Firebase SDK available. Firebase Storage initialized but not used in this story (client-side only). Infrastructure ready for future stories.">
        </package>
        <package name="openai" version="latest" reason="OpenAI SDK for Vision API integration. Needs to be installed: `npm install openai`. Use for GPT-4 Turbo Vision API calls in OCR API route. Server-side only (not exposed to client).">
        </package>
      </ecosystem>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="architecture">
      <name>API Route Structure</name>
      <description>OCR API route must be created in `app/api/ocr/route.ts` following Next.js App Router API routes pattern. Use POST method for image upload to API route. Return JSON response with extracted text or error message.</description>
      <source>docs/architecture.md#Project-Structure</source>
    </constraint>
    <constraint type="architecture">
      <name>OpenAI Client Utilities</name>
      <description>OpenAI client utilities must be created in `lib/openai/client.ts` (server-side only). OpenAI API key must be server-side (not exposed to client). Use `OPENAI_API_KEY` environment variable (not `NEXT_PUBLIC_` prefix).</description>
      <source>docs/architecture.md#Project-Structure</source>
    </constraint>
    <constraint type="architecture">
      <name>Next.js App Router</name>
      <description>API route must follow Next.js App Router patterns. Server-side API routes run on server (not client). Client components may need "use client" directive if using React hooks or browser APIs.</description>
      <source>docs/architecture.md#Project-Structure</source>
    </constraint>
    <constraint type="security">
      <name>API Key Security</name>
      <description>OpenAI API key must be server-side only (not exposed to client). Use `OPENAI_API_KEY` in `.env.local` (not `NEXT_PUBLIC_` prefix). Verify API key is loaded correctly in API route. Ensure API key is not exposed to client-side code.</description>
      <source>docs/stories/1-3-ocr-vision-llm-integration.md#Dev-Notes</source>
    </constraint>
    <constraint type="styling">
      <name>Tailwind CSS v4</name>
      <description>Use Tailwind CSS v4 utility classes for styling. Follow existing styling patterns from ImageUpload and TextInput components. Use theme variables for dark mode support.</description>
      <source>docs/stories/1-2-image-upload-interface.md#Dev-Agent-Record</source>
    </constraint>
    <constraint type="accessibility">
      <name>Accessibility Requirements</name>
      <description>OCR flow must maintain accessibility: loading indicators must be accessible (ARIA labels), error messages must be accessible (screen reader support), keyboard navigation support, proper semantic HTML. Follow WCAG guidelines for educational platform accessibility.</description>
      <source>docs/PRD.md#Accessibility-Requirements</source>
    </constraint>
    <constraint type="naming">
      <name>Naming Patterns</name>
      <description>Follow naming patterns: API routes (`route.ts` in `app/api/{endpoint}/` directory), Client utilities (`client.ts` in `lib/{service}/` directory), Functions (camelCase), Constants (UPPER_SNAKE_CASE), Types/Interfaces (PascalCase).</description>
      <source>docs/architecture.md#Naming-Patterns</source>
    </constraint>
    <constraint type="typescript">
      <name>TypeScript Strict Mode</name>
      <description>TypeScript strict mode enabled. Use proper type definitions for OCR API route, OpenAI client, component props, and state. Import alias `@/*` configured for path resolution.</description>
      <source>docs/stories/0-2-firebase-project-setup.md#Dev-Notes</source>
    </constraint>
    <constraint type="error-handling">
      <name>Error Handling</name>
      <description>Implement comprehensive error handling: OpenAI API errors (rate limits, network errors, invalid responses), retry logic with exponential backoff (up to 3 attempts), user-friendly error messages, fallback option to switch to text input if OCR fails, error logging (console for dev, Firebase Analytics for prod).</description>
      <source>docs/epics.md#Story-1.3</source>
    </constraint>
    <constraint type="state-management">
      <name>State Management</name>
      <description>Use React useState for OCR processing state: loading state (during OCR processing), extracted text state (from API response), error state (OCR errors). Manage OCR flow: image selection → upload to API → processing → extracted text → display in TextInput. Update ImageUpload component state to track OCR processing status.</description>
      <source>docs/stories/1-3-ocr-vision-llm-integration.md#Dev-Notes</source>
    </constraint>
    <constraint type="integration">
      <name>Component Integration</name>
      <description>ImageUpload component calls `/api/ocr` endpoint after image selection. OCR API route processes image and returns extracted text. Extracted text is displayed in TextInput component for review/editing. Main interface (`app/page.tsx`) coordinates OCR flow between ImageUpload and TextInput.</description>
      <source>docs/stories/1-3-ocr-vision-llm-integration.md#Dev-Notes</source>
    </constraint>
  </constraints>

  <interfaces>
    <interface name="OCR API Route" kind="Next.js API route" signature="POST /api/ocr" path="app/api/ocr/route.ts">
      <description>Next.js App Router API route that receives image from client, calls OpenAI Vision API (GPT-4 Turbo Vision), extracts text from image, and returns extracted text to client. Handles OpenAI API errors, implements retry logic with exponential backoff (up to 3 attempts), handles rate limit errors, network timeout errors, and invalid image format errors. Returns JSON response with extracted text or error message.</description>
    </interface>
    <interface name="OpenAI Client" kind="service" signature="OpenAI client with Vision API helper functions" path="lib/openai/client.ts">
      <description>OpenAI client setup and Vision API helper functions. Server-side only (not exposed to client). Uses `OPENAI_API_KEY` environment variable (not `NEXT_PUBLIC_` prefix). Provides helper function for Vision API calls with proper prompt for math problem extraction. Handles API key validation and error handling.</description>
    </interface>
    <interface name="ImageUpload Component" kind="React component" signature="ImageUpload component extended with OCR integration" path="components/problem-input/ImageUpload.tsx">
      <description>Image upload component extended with OCR integration. Calls `/api/ocr` endpoint after image selection. Sends selected image to API route. Displays loading indicator during OCR processing. Receives extracted text from API response. Updates component state to manage OCR flow (upload → processing → extracted text). Handles OCR errors and provides retry functionality.</description>
    </interface>
    <interface name="TextInput Component" kind="React component" signature="TextInput component that displays extracted text" path="components/problem-input/TextInput.tsx">
      <description>Text input component that displays extracted text from OCR for review/editing. Component accepts multi-line text input via textarea element. Component has inputValue state that can be set from OCR extracted text. Component allows student to edit extracted text before submitting.</description>
    </interface>
    <interface name="Main Interface" kind="Next.js page" signature="Home page component that coordinates OCR flow" path="app/page.tsx">
      <description>Main interface page that coordinates OCR flow between ImageUpload and TextInput components. ImageUpload calls OCR API → extracted text displayed in TextInput → student can edit and submit. Component manages OCR flow state and coordinates between components.</description>
    </interface>
    <interface name="React useState" kind="React hook" signature="useState&lt;string&gt;() for extracted text, useState&lt;boolean&gt;() for loading state, useState&lt;string | null&gt;() for error state" path="components/problem-input/ImageUpload.tsx">
      <description>React useState hook for managing OCR processing state. Use for extracted text state (from API response), loading state (during OCR processing), error state (OCR errors). Update ImageUpload component state to track OCR processing status.</description>
    </interface>
    <interface name="OpenAI Vision API" kind="API" signature="GPT-4 Turbo Vision API for OCR" path="lib/openai/client.ts">
      <description>OpenAI GPT-4 Turbo Vision API for OCR image processing. Accepts image files (base64 or file upload). Creates prompt specifically for math problem extraction. Extracts text from image while maintaining mathematical notation where possible. Returns extracted text in response.</description>
    </interface>
    <interface name="Fetch API" kind="browser API" signature="fetch('/api/ocr', { method: 'POST', body: FormData })" path="components/problem-input/ImageUpload.tsx">
      <description>Fetch API for calling OCR API route from ImageUpload component. Sends selected image file to `/api/ocr` endpoint using FormData. Handles network errors and retry logic. Receives JSON response with extracted text or error message.</description>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Testing standards for this story focus on verifying OCR functionality, API integration, error handling, and accessibility. Key verification points include: OCR API route processes images correctly, extracted text displays in text input field, loading indicator shows during OCR processing, extracted text maintains mathematical notation, student can edit extracted text before submitting, error handling works (API errors, rate limits, network timeouts, invalid images), retry logic works (up to 3 attempts with exponential backoff), fallback option to switch to text input works, error messages are user-friendly and actionable, errors are logged correctly (dev console or Firebase Analytics). Manual verification is appropriate for this foundational API integration story. API route testing should verify OpenAI Vision API integration, error handling, and retry logic. Component testing should verify OCR flow coordination between ImageUpload and TextInput. Accessibility testing should verify loading indicators, error messages, and keyboard navigation are accessible.
    </standards>
    <locations>
      <location>Manual verification (no test files yet for API integration story)</location>
      <location>Future tests will be co-located with API routes: `app/api/ocr/route.test.ts`</location>
      <location>Future tests will be co-located with components: `components/problem-input/ImageUpload.test.tsx`</location>
      <location>Integration tests: `app/**/*.test.tsx` (for page integration)</location>
    </locations>
    <ideas>
      <test id="ac1" criterion="AC: 1" description="Verify image is sent to Vision LLM API: Check ImageUpload component sends image to `/api/ocr` endpoint, verify API route receives image correctly, verify OpenAI Vision API is called with correct parameters, verify API key is loaded correctly (server-side only), verify image is sent in correct format (FormData or base64).">
      </test>
      <test id="ac2" criterion="AC: 2" description="Verify extracted text displays in text input field: Check extracted text is received from API response, verify TextInput component receives extracted text, verify extracted text is displayed in textarea, verify text is editable before submission.">
      </test>
      <test id="ac3" criterion="AC: 3" description="Verify loading indicator shows during OCR processing: Check loading state is set during OCR API call, verify loading indicator is visible during processing, verify loading indicator hides after OCR completes or fails, verify loading indicator is accessible (ARIA labels).">
      </test>
      <test id="ac4" criterion="AC: 4" description="Verify extracted text maintains mathematical notation: Check Vision API prompt includes instruction to preserve mathematical notation, verify extracted text contains mathematical symbols and notation, verify mathematical notation is displayed correctly in text input field.">
      </test>
      <test id="ac5" criterion="AC: 5" description="Verify student can edit extracted text before submitting: Check TextInput component allows editing extracted text, verify text input field is editable, verify student can modify extracted text before submission, verify modified text can be submitted.">
      </test>
      <test id="ac6" criterion="AC: 6" description="Verify error handling: Check OpenAI API errors are handled gracefully with user-friendly messages, verify retry logic works (up to 3 attempts with exponential backoff), verify rate limit errors show appropriate messaging and retry suggestion, verify network timeout errors show retry option, verify invalid image format errors (blurry, no text) show specific error message, verify error messages are user-friendly and actionable, verify fallback option to switch to text input works, verify errors are logged correctly (dev console or Firebase Analytics).">
      </test>
    </ideas>
  </tests>
</story-context>

